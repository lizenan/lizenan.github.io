

<!-- 
  IMPORTANT! 
  
  Keep this file unchanged to use as a template for all future project pages. 

  For every new project you add to your portfolio, make a copy of this file in the
  'project-pages' folder with a name related to the project.
-->


<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8">
    <!-- 
      TODO

      Upload your Unemployable (or whatever photo you like) to the assets/images folder
      and change the name of the image below to match the uploaded one

      Change the title in the <title> tag to whatever you would like the title of your portfolio to be

      This should be the same across all pages.
     -->
     <title>Human Pose Estimation</title>
    <meta name="description" content="A portfolio template for the Unemployables community.">
    <meta name="viewport" content="width=device-width, initial-scale=1" />

		<link rel="stylesheet" href="../css/layout.css">
    <link rel="stylesheet" href="../css/typography.css">
    <link rel="stylesheet" href="../css/utilities.css">
    <link rel="stylesheet" href="../css/utilities.css">

		<script defer src="../js/script.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
	</head>
	<body>
    <!-- NAVBAR -->
    <div class="navbar">
      <a class="nav-title-link" href="../index.html">
        <!-- 
          TODO - Change the "Portfolio Title" to whatever you want displayed in the top left

          (this should be the same across all pages)
         -->
        <span class="nav-title">Project</span>
        <!-- 
          TODO - Change the email after 'mailto:' to your email address for contact 
        
          (this should be the same across all pages)
        -->
        <a class="button" href="../index.html">
          <span class="button-text">Home Page</span>
        </a>
      </a>
    </div>

    <!-- MAIN PAGE CONTENT -->
    <div id="main-content">

      <!-- PROJECT HEADER -->
      <div id="project-header">
          <!-- 
            TODO

            - Change the 'main-title' text to the name of your project
            - Change the 'body-text' text to a short and sweet description of your project (maybe the same as the one on the project card)
            - Change "desktop.jpeg" to the image filename you uploaded in the assets/images folder.
          -->
        <div class="main-title">Human Pose Estimation</div>
        <div class="body-text"></div>
        <video class="project-header-image" src="../assets/images/A3D_2.mp4" autoplay controls muted>
      </div>

      <!-- PROJECT DETAILS -->
      <!-- 
        TODO

        - Change the 'subheader-text' to whatever header you want for project details
        - Add paragraphs using the <div class="body-text"></div> elements in the "project-details-content"
      -->
      <div id="project-details">
        <div class="project-details-content">
          <div class="subheader-text">1 Body Estimation</div>
        <div class="gallery-image-container">
        <img src="../assets/images/PE.svg" class="gallery-image">
        <span class="image-caption"><strong>Figure 1. PE structure</strong></span>
        </div>
        <div class="body-text">We evaluated several approaches for 3D Human Pose Estimation, including 3D lifting from 2D keypoints (both CNN-based and transformer-based), SMPL-based recovery methods (such as HMR and SMPLer-x), and direct 3D estimation techniques (such as stacked hourglass pose networks). SMPL-based recovery methods often suffer from long processing times, high computational costs, and minor positional offsets when mapping poses back to images. 3D lifting methods typically provide lower accuracy. On the other hand, direct 3D estimation methods frequently struggle with the lack of sufficient 3D ground truth data for training. After experimenting with various solutions, we chose direct 3D estimation for our product due to its accuracy, lower processing time requirements, and suitability for real-time mobile applications.</div>
        <div class="body-text">To address the shortage of ground truth 3D data and enhance the overall quality, we have enhanced our solution through several approaches:
        <ol>
          <li>Augmentation with Synthetic 3D Data: We incorporate synthetic 3D data generated from motion capture (mocap) systems to enrich our training dataset.</li>
          <li>Rotoscoping Tools: We use rotoscoping tools to obtain real 3D data from video footage, providing additional ground truth for training.</li>
          <li>Model Structure Improvements: We refine the architecture of the vanilla model to improve its performance and accuracy.</li>
          <li>Two-Stage Training: We employ a two-stage training approach to incrementally improve model accuracy and robustness.</li>
          <li>Integration of 2D and 3D Data: We use an end-to-end training strategy that organically combines 2D and 3D data to leverage the strengths of both modalities.</li>
          <li>Enhanced Weight Initialization: We implement advanced weight initialization techniques to facilitate better convergence during training.</li>
          <li>Score-Guided Diffusion Fine-Tuning: We apply score-guided diffusion methods for fine-tuning, which helps in refining model predictions and enhancing overall performance in depth dimension.</li>
        </ol>
        </div>
        <div class="subsubheader-text">1.1 Synthetic Data Augmentation</div>
        <div class="body-text">For the mocap data, we take the following steps to create a diverse and extensive synthetic dataset:
          <ol>
            <li>Retargeting and Rendering: We retarget mocap data to different character models, including male, female, and child figures, with various outfits and body shapes. This ensures a wide range of appearances and styles.</li>
            <li>Scene Setup: We create a variety of environments such as beaches, offices, parks, and factories, incorporating different lighting conditions to simulate real-world scenarios.</li>
            <li>Camera Angles: We use multiple camera angles to capture diverse perspectives of the scenes.</li>
            <li>Camera Extrinsic Matrix: By applying multiple camera extrinsic matrices to the mocap's world coordinates, we generate a large volume of synthetic 3D data paired with RGB images.</li>

          </ol>
        </div> 

        <div class="project-gallery-content">
          <div class="gallery-image-container half-width">
            <img src="../assets/images/synthetic_pe_data.png" class="gallery-image">
          </div>
          <div class="gallery-image-container half-width">
            <img src="../assets/images/synthetic_data_pe.png" class="gallery-image">
          </div>
          <span class="image-caption"><strong>Figure 2. 3D synthetic data exmaple</strong></span>
        </div>
        <div class="gallery-image-container half-width">
          <img src="../assets/images/synthetic_data_creation.png" class="gallery-image">
          <span class="image-caption"><strong>Figure 3. Scene creation</strong></span>
        </div>
        <div class="subsubheader-text">1.2 Rotoscoping Tool Data Augmentation</div>
        *We collect data from users who voluntarily sign an agreement allowing us to use their data to enhance our service quality.
        <div class="body-text">In our A3D pipeline, we utilize a 3D rotoscoping tool that allows users to correct inaccurate poses, providing additional ground truth for training.</div>
        <div class="gallery-image-container half-width">
        <img src="../assets/images/rotoscoping_data.png" class="gallery-image">
        <span class="image-caption"><strong>Figure 4. Rotoscoping Tool</strong></span>
      </div>
        <div class="subsubheader-text">1.3 Model Refinement</div>
        <div class="body-text">As shown in Figure 1, our experiments revealed that incorporating a classification block and using its loss as an auxiliary loss significantly improves the model's accuracy overall. Notably, for extreme cases such as high occlusion scenarios, our model demonstrates enhanced accuracy and stability in pose estimation compared to other models. Our joint training with the classification block achieves a 99.3% accuracy in detecting occluded joints. This precise additional metadata allows us to implement post-processing techniques that further enhance the performance of our product.</div>
        <div class="subsubheader-text">1.4 Training Strategy</div>
        <div class="body-text">After conducting experiments, we found that splitting the training into two stages can further enhance the model's accuracy. In the first stage, our second hourglass block estimates a 2D heatmap instead of a 3D heatmap. Once the 2D model converges effectively, we use it as the warm-up weights for the second stage. In this stage, we integrate a case-specific weight initialization solution. For the layer responsible for generating the final 3D heatmap, which has a different shape due to increased channel size compared to the 2D version, we do not discard the original weights. Instead, we adapt these weights to initialize the expanded channels, as depicted in Figure 5</div>
        <div class="body-text">For instance, in our 2D version(First stage of training), the weight shape of the 2D scoring layer is \([16, 1024,1,1]\) where \(16\) represents the number of joints, and the kernel size \([1,1]\) indicating that this convolutional layer can be treated as a fully connected layer. The 3D scoring layer's weight shape is \([16 \times 64, 1024,1,1]\), where \(64\) represents the depth dimension, reflecting the spatial meaning. </div>
        <div class="body-text">To initialize the weights for the 3D scoring layer, we first consider each joint's weight in the 2D model as \([1, 1024]\). We need to extend this to \([64, 1024]\) to match the depth dimension. To achieve this, we calculate each joint's depth distribution from our training dataset, then apply a softmax function to ensure the distribution sums to 1. We repeatly multiply \([1, 1024]\) with \(softmax[i]\), where \(i=0,2,3...63\), then concatenate the results together. Consequently, the final weight for each joint is extended from \([1, 1024]\) to \([64, 1024]\). In other words, if we apply  matrix multiplication between softmax result(\([1,64]\)) and 3D scoring layer(\([64,1024]\)), we will get a \([1,1024]\) Tensor all value equals to 2D scoring layer's weights.</div>
        <div class="gallery-image-container half-width">
        <img src="../assets/images/weight_initial.svg" class="gallery-image">
        <span class="image-caption"><strong>Figure 5. case-specific weight initialization</strong></span>
      </div>
        <div class="body-text">The results demonstrate that by initializing the weights using this method, the model converged significantly faster and achieved higher accuracy according to our evaluation metrics. This approach not only accelerates the training process but also enhances the model's overall performance.</div>
        <div class="subsubheader-text">1.5 Loss</div>
        <div class="body-text">To enhance the robustness of our final model in real-world scenarios, we combine both 2D and 3D data for the second stage of training. For 2D data, we compute only the 2D loss, while for 3D data, we calculate both 2D and 3D losses. Additionally, we integrate classification loss into the backpropagation process. This loss structure helps the model generalize better across different data types and improves its overall performance.</div>
        <div class="body-text">By training our model under this mechanism, it can generate highly accurate 3D keypoints for in-the-wild images. This approach ensures that the model handles diverse scenarios, including occlusions and challenging poses, making it robust and reliable for real-world applications.</div>
        <div class="subsubheader-text">1.6 Evaluaion Metrics</div>
        <div class="body-text">We tested our model using the Human3.6M test set under Protocol 1. The results showed improvements in performance, with higher accuracy in 2D and lower MPJPE in 3D pose estimation compared to previous approaches, as shown in table 1.</div>
        <div class="gallery-image-container">
          <img src="../assets/images/PE_metric.svg" class="gallery-image">
          <span class="image-caption"><strong>Table 1. Body PE metric</strong></span>
          </div>
        <div class="subsubheader-text">1.7 Score-guided diffusion fine-tuning</div>
        <div class="body-text">We found that ScoreHMR shows great potential in further improving the pose quality, particularly in providing more realistic depth estimation based on human 2D keypoints. Our process involves converting the 3D keypoints to 2D keypoints, along with providing the bounding box result. After obtaining SMPL-based vertex results, we convert these back into 3D keypoints. Using ScoreHMR, we fine-tune the original 3D keypoints with the provided 3D keypoint results. Below, we present an example illustrating the before and after effects of ScoreHMR fine-tuning on pose quality.</div>
        <div class="gallery-image-container">
          <img src="../assets/images/scoreHMR_finetune.svg" class="gallery-image">
          <span class="image-caption"><strong>Fugure 6. exmaple comparison between w and w/o scoreHMR</strong></span>
          </div>
        <div class="subsubheader-text">1.8 Distriminator</div>
        <div class="body-text">We developed a discriminator specifically for pose quality assessment. The discriminator's output score serves as a realism judgment tool, helping to evaluate and ensure the generated poses align with realistic human movements. This score allows us to identify and refine poses that may appear unnatural or deviate from expected motion patterns.</div>
        <div class="gallery-image-container half-width">
          <img src="../assets/images/discriminator.svg" class="gallery-image">
          <span class="image-caption"><strong>Figure 7. Distriminator Training</strong></span>
          </div>
        <div class="subsubheader-text">1.9 Mobile Version</div>
        <div class="body-text">We implemented distillation learning to train a smaller, lightweight model capable of running in real-time on mobile devices.</div>
        </div>
        <div class="gallery-image-container half-width">
        <video class="img-fluid" autoplay controls muted>
          <source src="../assets/images/mobile_version.mp4" type="video/mp4">
        </video>
      </div>
        <div class="subheader-text">2 Face estimation</div>
        <div class="gallery-image-container">
        <img src="../assets/images/face_estimation.svg" class="gallery-image">
        <span class="image-caption"><strong>Figure 8. face estimation pipeline</strong></span>
        </div>
        <div class="body-text">For face estimation, we employ backbonds both face detection and facial landmark prediction, similar to our body estimation solution. The process starts with the face detector, which locates the human face in the image. Afterward, we use a facial landmark prediction model to obtain the key face landmarks. Each facial landmark is pre-matched to a vertex in our 4DFM model.</div>
        <div class="body-text">We solve this using a standard 3D Morphable Model (3DMM) fitting process as follows:
          <ol>
            <li>Camera Parameters Estimation: Solve camera parameters using 2D landmarks and corresponding 3D vertices.</li>
            <li>Shape Component Estimation: Apply the camera parameters to the 3D vertices and solve for the basic shape components with 2D landmarks</li>
            <li>Expression Blendshapes: Solve for the expression blendshapes using 3D vertices from 2. and 2D landmarks</li>
            <li>Refinement: Repeat steps 1–3 for refinement over several iterations.</li>
            <li>Blendshape Weights: Obtain the blendshape weights from the refined model.</li>
            <li>Retargeting: Retarget the blendshape weights to a target face model.</li>
            <li>Application: Apply the blendshape weights to the target face model for the final result.</li>
          </ol>
        </div>
        <div class="project-gallery-content">
        <div class="gallery-image-container half-width">
          <img style="height:400px;" src="../assets/images/landmarks.png" class="gallery-image">
        </div>
        <div class="gallery-image-container half-width">
          <img style="height:400px;" src="../assets/images/4DFM_matched_vectices.png" class="gallery-image">
        </div>
        <span class="image-caption"><strong>Figure 9. landmarks and their matched vectices in 4DFM</strong></span>
      </div>
        <div class="body-text">Our face landmark prediction model contain 106 keypoints including eyes and eyelids. Below is our face estimation result.</div>
        <video class="img-fluid" autoplay controls muted>
          <source src="../assets/images/face_estimation_example.mp4" type="video/mp4">
        </video>
        
        <div class="subheader-text">3 Fingers Estimation</div>
        <div class="gallery-image-container">
          <img src="../assets/images/hand_estimation_pipeline.drawio.svg" class="gallery-image">
          <span class="image-caption"><strong>Figure 10. hand estimation pipeline</strong></span>
          </div>
        <div class="body-text">For finger estimation, the hand detector and keypoint prediction model share a similar structure to our body estimation model. It is important to note that distinguishing between left and right hands using only a hand detector is challenging. Therefore, we utilize the body estimation results to accurately assign bounding boxes to the left and right hands.</div>
        <div class="gallery-image-container half-width">
          <img src="../assets/images/hand_keypoints.png" class="gallery-image">
          <span class="image-caption"><strong>Figure 11. hand keypoints</strong></span>
          </div>
        <video class="img-fluid" autoplay controls muted>
          <source src="../assets/images/hand_estimation_example.mp4" type="video/mp4">
        </video>
        
        <div class="subheader-text">4 Multi-person Estimation</div>
        <div class="body-text">Our multi-person pose estimation uses a bottom-up approach, following the same process as single-person pose estimation as outlined in Section 1, 2, and 3. The only addition is the inclusion of a person ID as metadata during pose generation. For details on the multi-person tracking solution, please refer to the section on <a href="./Object-Detection-Tracking.html">#object detection and tracking</a></div>

        <div class="subheader-text">5 Some Interesting Experiments we also done</div>
        <div class="body-text">We also explore several ideas to potentially enhance pose quality:
          <ol>
            <li>Motion deblurring</li>
            <li>Segmentation prior to pose estimation</li>
            <li>Using 20-joint models for improved calculation of hands and feet rotations</li>
            <li>Prediction focused solely on the lower body</li>
            <li>Pose estimation for animals</li>
          </ol>
        </div>
        <div class="subsubheader-text">5.1 Motion deblurring</div>
        <div class="body-text">As Figure 8 shown, heatmaps can be mismatched in images affected by motion blur. To address this issue, we employed DeblurGAN, an example is depicted in Figure 9. However, as we can see, the current state-of-the-art motion deblurring models did not meet our expectations for these use cases. This approach remains a potential avenue for future improvement</div>
        <div class="project-gallery-content">
        <div class="gallery-image-container half-width">
          <img src="../assets/images/motion_blur.svg" class="gallery-image">
          <span class="image-caption"><strong>Figure 12. heatmap from motion blurred frame</strong></span>
        </div>
        <div class="gallery-image-container half-width">
          <img src="../assets/images/motion_deblur.png" class="gallery-image">
          <span class="image-caption"><strong>Figure 13. Deblured result on the right</strong></span>
        </div>
      </div>
        <div class="subsubheader-text">5.2 Background Processing prior to Pose Estimation</div>
        <div class="body-text">The idea was to reduce unrelated information, or noise, from the background of an image to achieve more accurate predictions. However, we did not observe significant improvements with this approach. Additionally, it increased processing time.</div>
        <div class="project-gallery-content">
        <div class="gallery-image-container half-width">
          <img src="../assets/images/background_substraction.png" class="gallery-image">
        </div>
        <div class="gallery-image-container half-width">
          <img src="../assets/images/background_info.png" class="gallery-image">
        </div>
        <span class="image-caption"><strong>Figure 14. provide background information as one of the input to PE model</strong></span>
      </div>
        <video class="img-fluid" autoplay controls muted>
          <source src="../assets/images/segmentation.mp4" type="video/mp4">
          
        </video>
        <span class="image-caption"><strong>background removal</strong></span>
        <div class="subsubheader-text">5.3 Additional Joints</div>
        <div class="body-text">Due to the smaller dataset containing additional joints, the overall accuracy may be affected.</div>
        <div class="gallery-image-container half-width">
          <img style="height:500px;width:300px;" src="../assets/images/20_joints.png" class="gallery-image">
          <span class="image-caption"><strong>Figure 15. 20 joints</strong></span>
        </div>
        <div class="subsubheader-text">5.4 Lower Body</div>
        <div class="gallery-image-container half-width">
          <img src="../assets/images/lower_body.png" class="gallery-image">
          <span class="image-caption"><strong>Figure 16. lower body</strong></span>
        </div>
        <div class="subsubheader-text">5.5 Pose Estimation for animals</div>
        <div class="gallery-image-container half-width">
          <img src="../assets/images/animal_pe.png" class="gallery-image">
          <span class="image-caption"><strong>Figure 17. PE for dog</strong></span>
        </div>
        <div class="subheader-text">6 Reference</div>
        <div class="body-text">Newell, A., Yang, K., & Deng, J. (2016, July 26). Stacked Hourglass Networks for Human Pose Estimation. arXiv.org. https://arxiv.org/abs/1603.06937  </div>
        <div class="body-text">Stathopoulos, A., Han, L., & Metaxas, D. (2024, March 14). Score-guided diffusion for 3D human recovery. arXiv.org. https://arxiv.org/abs/2403.09623 </div>
        <div class="body-text">Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black, M. J., (2015, October 26). SMPL: A skinned multi-person linear model: ACM Transactions on Graphics: VOL 34, no 6. ACM Transactions on Graphics. https://dl.acm.org/doi/10.1145/2816795.2818013 </div>
        <div class="body-text">Cai, Z., Yin, W., Zeng, A., Wei, C., Sun, Q., Wang, Y., Pang, H. E., Mei, H., Zhang, M., Zhang, L., Loy, C. C., Yang, L., & Liu, Z. (2024, July 28). SMPLer-X: Scaling up expressive human pose and shape estimation. arXiv.org. https://arxiv.org/abs/2309.17448 </div>
        <div class="body-text">Zhou, F., Yin, J., & Li, P. (2023, December 25). Lifting by image -- leveraging image cues for accurate 3D human pose estimation. arXiv.org. https://arxiv.org/abs/2312.15636  </div>
        <div class="body-text">Pavlakos, G., Zhou, X., Derpanis, K. G., & Daniilidis, K. (2017, July 26). Coarse-to-fine volumetric prediction for single-image 3D human pose. arXiv.org. https://arxiv.org/abs/1611.07828  </div>
        <div class="body-text">Kupyn, O., Martyniuk, T., Wu, J., & Wang, Z. (2019, August 10). DeblurGAN-V2: Deblurring (orders-of-magnitude) faster and better. arXiv.org. https://arxiv.org/abs/1908.03826 </div>
      </div>
      </div>


      

      

    <!-- FOOTER -->
    <div id="footer">
      <!-- 
        TODO - Change href to your Instagram account (can also delete entire "a" element if no Instagram) 

        This should be the same across all pages.
      -->
      <a class="icon-link" target="_blank" href="https://www.linkedin.com/in/chrislizenan/">
        <image src="../assets/icons/linkedin-svgrepo-com.svg" class="footer-icon"/>
      </a>
      <!-- 
        TODO - Change href to your Twitter account (can also delete entire "a" element if no Twitter) 
      
        This should be the same across all pages.
      -->
      <!-- 
        TODO - Change the email after "mailto" to your contact email 
      
        This should be the same across all pages.
      -->
      <a class="icon-link" href="mailto:zenanlicareer@gmail.com">
        <image src="../assets/icons/mail.svg" class="footer-icon"/>
      </a>
    </div>

	</body>
</html>
