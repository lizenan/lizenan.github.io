

<!-- 
  IMPORTANT! 
  
  Keep this file unchanged to use as a template for all future project pages. 

  For every new project you add to your portfolio, make a copy of this file in the
  'project-pages' folder with a name related to the project.
-->


<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8">
    <!-- 
      TODO

      Upload your Unemployable (or whatever photo you like) to the assets/images folder
      and change the name of the image below to match the uploaded one

      Change the title in the <title> tag to whatever you would like the title of your portfolio to be

      This should be the same across all pages.
     -->
     <title>T2M-model-improvements</title>
    <meta name="description" content="A portfolio template for the Unemployables community.">
    <meta name="viewport" content="width=device-width, initial-scale=1" />

		<link rel="stylesheet" href="../css/layout.css">
    <link rel="stylesheet" href="../css/typography.css">
    <link rel="stylesheet" href="../css/utilities.css">
    <link rel="stylesheet" href="../css/utilities.css">

		<script defer src="../js/script.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
	</head>
	<body>
    <!-- NAVBAR -->
    <div class="navbar">
      <a class="nav-title-link" href="../index.html">
        <!-- 
          TODO - Change the "Portfolio Title" to whatever you want displayed in the top left

          (this should be the same across all pages)
         -->
        <span class="nav-title">Project</span>
        <!-- 
          TODO - Change the email after 'mailto:' to your email address for contact 
        
          (this should be the same across all pages)
        -->
        <a class="button" href="../index.html">
          <span class="button-text">Home Page</span>
        </a>
      </a>
    </div>

    <!-- MAIN PAGE CONTENT -->
    <div id="main-content">

      <!-- PROJECT HEADER -->
      <div id="project-header">
          <!-- 
            TODO

            - Change the 'main-title' text to the name of your project
            - Change the 'body-text' text to a short and sweet description of your project (maybe the same as the one on the project card)
            - Change "desktop.jpeg" to the image filename you uploaded in the assets/images folder.
          -->
        <div class="main-title">T2M-model-improvements</div>
        <div class="body-text"></div>
        <video class="project-header-image" src="../assets/images/saymotion_improvements.mp4" autoplay controls muted>
      </div>

      <!-- PROJECT DETAILS -->
      <!-- 
        TODO

        - Change the 'subheader-text' to whatever header you want for project details
        - Add paragraphs using the <div class="body-text"></div> elements in the "project-details-content"
      -->
      <div id="project-details">
        <div class="project-details-content">
          <div class="body-text">*In this article, I will focus on the key improvements I have led, though these are not all the enhancements we have made to the product.</div>
        </div>
        <div class="subheader-text">1 Temporal-Aware Text Encoder</div>
        <div class="gallery-image-container half-width">
        <img src="../assets/images/sequential-aware.png" class="gallery-image">
        <span class="image-caption"><strong>Figure 1. Text Tokens Through Cross Attention</strong></span> 
        </div>
        <div class="body-text">In most state-of-the-art models, such as MDM, MLM, MoMask, and T2M-GPT, the CLIP text encoder is commonly used as a frozen pre-trained text encoder. However, a significant issue with CLIP's text encoder is that, to align text and visual embedding spaces, it sacrifices sequential information in favor of emphasizing spatial relations. Since images contain only spatial information, this approach can lead to a loss of important sequential context in text data.</div>
        <div class="body-text">Motion data, which is inherently sequential, exacerbates this issue. For example, when a description contains multiple actions, such as "a man doing a cartwheel and running away," the model may struggle to generate the "running away" part or, in some cases, may reverse the action order, producing "running away" before "doing a cartwheel."</div> 
        <div class="body-text">To address this issue, we have experimented with replacing the text encoder with a more powerful interpreter, such as MPNet, and explored using the full token sequence via cross-attention layers. as shown in Figure 1.</div>
        <div class="body-text">After implementing these changes, we observed the following improvements:</div>
        <video class="img-fluid" autoplay controls muted>
          <source src="../assets/images/temporal-aware.mp4" type="video/mp4">
        </video>

        <div class="subheader-text">3 LLM+RAG for prompt optimization</div>
        <img src="../assets/images/LLM+RAG-prompt-optimizer.svg" class="gallery-image">
        <span class="image-caption"><strong>Figure 2. Prompt Optimizer Pipeline</strong></span>
        <div class="body-text">In practice, users' inputs can be unpredictable, and it is unrealistic to expect them to provide informative, well-organized prompts free of typos. We have extracted some user prompts and analyzed the differences between real-world user prompts and the ground truth prompts used to train our model.</div>
        <div class="body-text">To address this issue, we leverage LLM models to correct typos and translate user prompts into English. We then use MPNet to convert these corrected prompts into embedding space and search our vectorDB to find the closest match in the training data. The vectorDB stores all vectorized training prompts.</div>
        <div class="body-text">We are not using this as a direct retrieval function. Instead of directly replacing the user's prompt with the closest match, we use the retrieved prompt as information to instruct the LLM to rewrite the user's prompt. This rewritten prompt maintains the same style as the retrieved sentence while ensuring that the actions align with the user's original prompt.</div>
        <div class="body-text">After implementing these changes, we observed the following improvements:</div>
        <video class="img-fluid" autoplay controls muted>
          <source src="../assets/images/LLM+RAG.mp4" type="video/mp4">
        </video>

        <div class="subheader-text">4 RAG during motion Inference</div>
        <img src="../assets/images/motionRAG_stucture.svg" class="gallery-image">
        <span class="image-caption"><strong>Figure 3. motionRAG pipeline in different settings</strong></span> 
          <div class="body-text">In some cases, our model may misinterpret the prompt action and produce motions that do not match semantically. To reduce such occurrences, we can apply RAG to retrieve a segment of motion as a hint for the motion generator, guiding it to generate motions that align with the provided context.</div>
          <div class="body-text">There are several combinations we need to test for RAG-guided generation:
            <ol>
              <li>Which model should we use for motion retrieval?</li>
              <li>Should we treat the retrieved tokens as replaceable?</li>
              <li>How should we integrate the retrieved tokens into the generation process?</li>
            </ol>
          </div>
          <div class="body-text">To address these questions, we designed our experiments as follows:
            <ol>
              <li>Model Comparison: We compared the performance of using MPNet's text-to-text similarity matrix to retrieve motion sequences from the database with using TMR's text-to-motion similarity matrix for the same purpose.</li>
              <li>Token Replacement: We created mask matrices to test whether retrieved tokens should be treated as replaceable or not, and compared the performance of these approaches.</li>
              <li>Token Integration: We experimented with two methods for integrating retrieved tokens: segment sampling and interval insert sampling. We used the transformed similarity score as the sampling ratio to determine the retrieval length.</li>
            </ol>
          </div>
          <div class="body-text">sampling ratio formula is shown as below:</div>
          <div>
            \[ y = \frac{1}{1 + e^{-k(x - 0.5)}} \]
          </div>
          <div class="body-text">where \(k\) is constant, \(x\) is similarity score where range is \( [0, 1] \). \(e\) is the base of the natural logarithm</div>
          <div class="body-text">The graph is illustrated in Figure 4.</div>
          <div class="center-gallery-image-container center-half-width" >
          <img src="../assets/images/scoring-graph.png" class="gallery-image">
          <span class="image-caption"><strong>Figure 4. sampling graph based on similarity value</strong></span>
          </div>
          <div class="body-text">We selected the best combination from all tested options and present its improvement results below.</div>
          <video class="img-fluid" autoplay controls muted>
            <source src="../assets/images/motionRAG1.mp4" type="video/mp4">
          </video>
          <video class="img-fluid" autoplay controls muted>
            <source src="../assets/images/motionRAG2.mp4" type="video/mp4">
          </video>
          
          <div class="subheader-text">5 Using beam search on Motion Variant Generation</div>
          <div class="body-text">In product, we provide variant motion generation results, we apply beam search to provide partial variants.</div>
          
        <div class="subheader-text">6 Reference</div>
        <div class="body-text">Team, G., Georgiev, P., Lei, V. I., Burnell, R., Bai, L., Gulati, A., Tanzer, G., Vincent, D., Pan, Z., Wang, S., Mariooryad, S., Ding, Y., Geng, X., Alcober, F., Frostig, R., Omernick, M., Walker, L., Paduraru, C., Sorokin, C., … Vinyals, O. (2024, August 8). Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv.org. https://arxiv.org/abs/2403.05530 </div>
        <div class="body-text">Guo, C., Mu, Y., Javed, M. G., Wang, S., & Cheng, L. (2023, November 29). Momask: Generative masked modeling of 3D human motions. arXiv.org. https://arxiv.org/abs/2312.00063 </div>
        <div class="body-text">Zhang, J., Zhang, Y., Cun, X., Huang, S., Zhang, Y., Zhao, H., Lu, H., & Shen, X. (2023, September 24). T2M-GPT: Generating human motion from textual descriptions with discrete representations. arXiv.org. https://arxiv.org/abs/2301.06052 </div>
        <div class="body-text">Song, K., Tan, X., Qin, T., Lu, J., & Liu, T.-Y. (2020, November 2). MPNet: Masked and permuted pre-training for Language Understanding. arXiv.org. https://arxiv.org/abs/2004.09297 </div>
        <div class="body-text">Petrovich, M., Black, M. J., & Varol, G. (2023, August 25). TMR: Text-to-motion retrieval using contrastive 3D human motion synthesis. arXiv.org. https://arxiv.org/abs/2305.00976 </div>
      </div>
      </div>


      

      

    <!-- FOOTER -->
    <div id="footer">
      <!-- 
        TODO - Change href to your Instagram account (can also delete entire "a" element if no Instagram) 

        This should be the same across all pages.
      -->
      <a class="icon-link" target="_blank" href="https://www.linkedin.com/in/chrislizenan/">
        <image src="../assets/icons/linkedin-svgrepo-com.svg" class="footer-icon"/>
      </a>
      <!-- 
        TODO - Change href to your Twitter account (can also delete entire "a" element if no Twitter) 
      
        This should be the same across all pages.
      -->
      <!-- 
        TODO - Change the email after "mailto" to your contact email 
      
        This should be the same across all pages.
      -->
      <a class="icon-link" href="mailto:zenanlicareer@gmail.com">
        <image src="../assets/icons/mail.svg" class="footer-icon"/>
      </a>
    </div>

	</body>
</html>
