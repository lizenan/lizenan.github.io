

<!-- 
  IMPORTANT! 
  
  Keep this file unchanged to use as a template for all future project pages. 

  For every new project you add to your portfolio, make a copy of this file in the
  'project-pages' folder with a name related to the project.
-->


<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8">
    <!-- 
      TODO

      Upload your Unemployable (or whatever photo you like) to the assets/images folder
      and change the name of the image below to match the uploaded one

      Change the title in the <title> tag to whatever you would like the title of your portfolio to be

      This should be the same across all pages.
     -->
     <title>MLLM-guided data synthesis</title>
    <meta name="description" content="A portfolio template for the Unemployables community.">
    <meta name="viewport" content="width=device-width, initial-scale=1" />

		<link rel="stylesheet" href="../css/layout.css">
    <link rel="stylesheet" href="../css/typography.css">
    <link rel="stylesheet" href="../css/utilities.css">
    <link rel="stylesheet" href="../css/utilities.css">

		<script defer src="../js/script.js"></script>
	</head>
	<body>
    <!-- NAVBAR -->
    <div class="navbar">
      <a class="nav-title-link" href="../index.html">
        <!-- 
          TODO - Change the "Portfolio Title" to whatever you want displayed in the top left

          (this should be the same across all pages)
         -->
        <span class="nav-title">Project</span>
        <!-- 
          TODO - Change the email after 'mailto:' to your email address for contact 
        
          (this should be the same across all pages)
        -->
        <a class="button" href="../index.html">
          <span class="button-text">Home Page</span>
        </a>
      </a>
    </div>

    <!-- MAIN PAGE CONTENT -->
    <div id="main-content">

      <!-- PROJECT HEADER -->
      <div id="project-header">
          <!-- 
            TODO

            - Change the 'main-title' text to the name of your project
            - Change the 'body-text' text to a short and sweet description of your project (maybe the same as the one on the project card)
            - Change "desktop.jpeg" to the image filename you uploaded in the assets/images folder.
          -->
        <div class="main-title">MLLM-guided data synthesis</div>
        <div class="body-text"></div>
        <image class="project-header-image" src="../assets/images/tri-modality-design.png">
      </div>

      <!-- PROJECT DETAILS -->
      <!-- 
        TODO

        - Change the 'subheader-text' to whatever header you want for project details
        - Add paragraphs using the <div class="body-text"></div> elements in the "project-details-content"
      -->
      <div id="project-details">
        <div class="subheader-text">1 Introduction</div>
        <div class="project-details-content">
          <div class="body-text">One of the biggest challenges in the human motion generation domain is the shortage of data. The largest text-to-motion dataset in the research field, HML3D, contains only <i>14,616</i> motion clips and <i>44,970</i> descriptions using <i>5,371</i> distinct words. Even with data augmentation techniques, such as mirroring motions, the increase in diversity is minimal. This small dataset often leads to overfitting in general motion generation models. Leveraging our Animate 3D product, we can generate high-quality motion data. Thanks to advancements in large language models (LLMs) and multimodal LLMs (MLLMs), we are now able to create a massive and diverse synthetic dataset for text-to-motion generation tasks. Our privately owned dataset contains over <strong><i>344,476</i></strong> motion clips and descriptions, utilizing <strong><i>12,139</i></strong> distinct words.</div>
        </div>
        <div class="subheader-text">2 Data Analysis for HumanML3D dataset</div>
        <div class="subsubheader-text">2.1 Similarity Test
        </div>
        <div class="body-text">As shown in Table 1, inspired by Text-Motion-Retrieval (TMR), we use MPNet to calculate the text description similarity between the training and test data. If a text in the training data has a similarity score of 90% or 95% or higher, we consider this an indication that a very similar semantic motion was used to train the model. As depicted in Table 1, such high text similarity between the training and test sets can potentially lead to overfitting, particularly in real-world scenarios.</div>
           <img src="../assets/images/HML3D_Comparison.svg" class="gallery-image">
           <span class="image-caption"><strong>Table 1</strong></span> 
           <div class="subsubheader-text">2.2 Action Distribution
          </div>
          <div class="body-text">We utilize the Part-of-Speech(POS) key from the HML3D ground truth data to visualize the action classes in the HML3D dataset. For improved clarity, we display only the top 30 classes in a bar chart and the top 50 in a pie chart. Figures 1 and 2 illustrate the action distribution in HumanML3D.</div>
          <div class="project-gallery-content">
          <div class="gallery-image-container half-width">
            <img src="../assets/images/HML3D_bar.png" class="gallery-image">
           <span class="image-caption"><strong>Figure 1</strong></span> 
          </div>
        
          <div class="gallery-image-container half-width">
           <img src="../assets/images/HML3D_pie.png" class="gallery-image">
           <span class="image-caption"><strong>Figure 2</strong></span> 
          </div>
          </div>
           <div class="subheader-text">3 Data Preparation for DM dataset</div>
          <div class="subsubheader-text">3.1 Video-Motion data creation</div>
        <div class="body-text">*Our data is collected from A3D users who have voluntarily granted us permission to use their data to enhance the quality of our services.
        </div>
        <div class="body-text">Within our Animate 3D pipeline, we apply multiple self-evaluation methods, including a pose realism discriminator and a high-pass score for joint movement changes over time. We set thresholds to maintain action diversity while ensuring motion quality. Since our A3D pipeline also supports upper-body mode (with a focus on facial and finger tracking) and multi-person mode, we first filter out these specific samples from the dataset. After this process, we obtain a relatively high-quality video-motion dataset.</div>
        <div class="subsubheader-text">3.2 Video-Text data creation</div>
        <div class="body-text">We experimented with several MLLM/multimodal models, including Gemini 1.5, VideoChat2, MPlug2, and Valor. Additionally, we explored various hyperparameter tuning strategies and prompt engineering techniques to generate diverse outputs from each model. InternVideo was used to calculate the video-to-text retrieval Top-1 recall score for each model. We also conducted a quantitative evaluation based on user preferences. Figure 3 provides an example of our survey, and Table 2 presents the best video-to-text retrieval Top-1 recall scores for each model. Feedback from 17 users was collected through the survey is shown in Table 3.</div>
           <img style="height:300px;" src="../assets/images/MLLM_R1.svg" class="gallery-image">
           <span class="image-caption"><strong>Table 2</strong></span> 
           <img src="../assets/images/survey_example.png" class="gallery-image">
           <span class="image-caption"><strong>Figure 3. User Preference Survey Example Question</strong></span> 
           <img style="height:300px;" src="../assets/images/MLLM_user_preference.svg" class="gallery-image">
           <span class="image-caption"><strong>Table 3. User Preference Results</strong></span> 
           <div class="body-text">As shown, Gemini 1.5 emerged as the clear winner, which led us to choose it for generating our video-text data.</div>
        <div class="subheader-text">4 Data Analysis for DM dataset</div>
        <div class="subsubheader-text">4.1 Action Keyword Distribution</div>

        <div class="body-text">We applied the POS algorithm to the generated texts and analyzed their distribution. As shown in Figures 4 and 5, the most significant difference between the HumanML3D dataset and ours is that the top action class in our dataset is <i>Dance</i>, while HumanML3D is <i>Walk</i>.
        </div>
        <div class="project-gallery-content">
          <div class="gallery-image-container half-width">
        <img src="../assets/images/DM_bar.png" class="gallery-image">
           <span class="image-caption"><strong>Figure 3</strong></span> 
           </div>
           <div class="gallery-image-container half-width">
           <img src="../assets/images/DM_pie.png" class="gallery-image">
           <span class="image-caption"><strong>Figure 4</strong></span> 
           </div>
          </div>
           <div class="subsubheader-text">4.2 Action embedding Distribution</div>
        <div class="body-text">We aim to gain a deeper understanding of the relationship between embedding distributions and keyword distributions. First, we use TMR to convert motion and text into embedding space. After applying Principal component analysis(PCA), we visualize the relationship between text embeddings and visual embeddings to examine the modality alignment in a well-trained TMR model. We observed that there is a noticeable modality gap between text embeddings and motion embeddings.
        </div>
        <img style="height:400px; width:550px;" src="../assets/images/text_motion_embeddings.png" class="gallery-image">
           <span class="image-caption"><strong>Figure 5</strong></span> 
           <div class="body-text">We also visualize the distribution of four keyword classes in both text and motion embedding spaces. Since the data points are transformed and reduced to 2D space, we believe that their distribution would be more distinguishable in the original dimensional space.</div>
           <div class="project-gallery-content">
            <div class="gallery-image-container half-width">
           <img src="../assets/images/POS_motion_embeddings.png" class="gallery-image">
           <span class="image-caption"><strong>Figure 7.keyword classes distribution in Motion embedding space</strong></span>
            </div>
            <div class="gallery-image-container half-width">
           <img src="../assets/images/POS_text_embeddings.png" class="gallery-image">
           <span class="image-caption"><strong>Figure 8.keyword classes distribution in Text embedding space</strong></span>
          </div>
          </div>
           <div class="subsubheader-text">4.3 Text and Motion metric insight</div>
           <div class="body-text">We evaluated the performance of TMR for text-to-motion and motion-to-text retrieval on our dataset. The results fall within the expected range, which supports the effectiveness of our synthetic dataset.</div>
           <img src="../assets/images/TMR-M2T.svg" class="gallery-image">
           <span class="image-caption"><strong>Table 3.Motion-Text Retrieval</strong></span>
           <img src="../assets/images/TMR-T2M.svg" class="gallery-image">
           <span class="image-caption"><strong>Table 4.Text-Motion Retrieval</strong></span>
           <div class="subheader-text">5 Clustering</div>
        <div class="body-text">
          we identified that the data is not equally distributed. This imbalance could potentially harm the model‚Äôs performance if certain types of motions dominate the dataset. To address this bias, we decided to perform clustering to group similar types of actions together. This approach allows us to set different sampling rates for different clusters during training, mitigating the impact of dataset imbalance.
          </div>
          <div class="body-text">To achieve this, we first applied DBSCAN and K-means clustering algorithms. Based on the results, K-means yielded better performance, so we used K-means for the initial step of our clustering process.</div>
          <div class="body-text">Due to the characteristics of K-means, more centroids are clustered in denser areas to ensure that each cluster maintains a similar number of data points. This results in varying average distances from the centroid for each cluster. To achieve a more varied and representative final result, where each class is more distinct, we used hierarchical clustering to obtain an unbalanced clustering outcome. This approach helps group similar or repeated actions together more effectively.
            </div>
            <div class="body-text">
              To address the imbalance in the A3D dataset, we initially avoided hierarchical clustering due to its high computational expense and time consumption. With a total of 380,000 data points, hierarchical clustering requires computing an 
              ùëõ
              √ó
              ùëõ
              distance matrix for each layer, which is impractical. Instead, we applied hierarchical clustering only to the centroids obtained from K-means, significantly reducing computation time. We fine-tuned the threshold to generate a final set of 12 clusters, ensuring that each cluster maintains semantic coherence while meeting the specified requirements.</div>
            <div class="body-text">We need to evaluate whether text embeddings or motion embeddings provide better clustering results. Figure 9 presents examples of clustering results for both motion and text embeddings. In these examples, motion clustering tends to focus on lower-level semantic motions, such as "raising hands in front of the chest and walking," which includes various actions with similar textual meanings like "zombie walk," "dumbbell lift walking," and "both hands shooting."</div>

              <div class="body-text">Conversely, text clustering results, as shown in Figure 10 with the example of "sword fight," reveal a broader range of movements that share the same textual meaning.</div>
              <img src="../assets/images/zombie_walk.png" class="gallery-image">
           <span class="image-caption"><strong>Figure 9. clustering based on motion embeddings</strong></span>
           <img src="../assets/images/sword_fight.png" class="gallery-image">
           <span class="image-caption"><strong>Figure 10. clustering based on text embeddings</strong></span>
        <div class="body-text">Ultimately, we chose text embeddings for clustering. Figure 11 displays the centroids of the 12 clusters across the entire sample space, while Figure 12 shows the data size for each cluster.</div>
          <div class="project-gallery-content">
            <div class="gallery-image-container half-width">
          <img src="../assets/images/hierarchical_centroids_in_all_samples.png" class="gallery-image">
           <span class="image-caption"><strong>Figure 11</strong></span>
           </div>
           <div class="gallery-image-container half-width">
           <img src="../assets/images/clustering_results.png" class="gallery-image">
           <span class="image-caption"><strong>Figure 12</strong></span>
          </div>
          </div>
        <div class="subheader-text">6 Evaluation Metrics</div>
        <img src="../assets/images/metric_data_synthesis.svg" class="gallery-image">
        <span class="image-caption"><strong>Table 5</strong></span>  
        <div class="body-text">We tested our model on the HML3D test set for a clearer comparison. The results show that incorporating the DM dataset can futher improve the model's performance in HumanML3D dataset.</div>
        <div class="body-text">It is important to note that we replaced the text encoder and motion encoder in the original T2M paper with TMR for a more precise evaluation. As a result, the numbers reported are not directly comparable to those in state-of-the-art models' papers.</div>
        <div class="subheader-text">7 Visualization Results</div>
        <video class="img-fluid" autoplay controls muted>
          <source src="../assets/images/data_synthesis_comparison1.mp4" type="video/mp4">
        </video>
        <video class="img-fluid" autoplay controls muted>
          <source src="../assets/images/data_synthesis_comparison2.mp4" type="video/mp4">
        </video>
        
        <div class="subheader-text">8 Conclusion</div>
        <div class="body-text">Using MLLM-guided synthetic data, grouping data based on semantic meaning, and applying a weighted sampling ratio can significantly enhance motion generation accuracy.</div>
        
        <div class="subheader-text">9 Reference</div>
        <div class="body-text">Chen, S., He, X., Guo, L., Zhu, X., Wang, W., Tang, J., & Liu, J. (2023, April 17). Valor: Vision-audio-language omni-perception pretraining model and dataset. arXiv.org. https://arxiv.org/abs/2304.08345 </div>
        <div class="body-text">Wang, Y., Li, K., Li, Y., He, Y., Huang, B., Zhao, Z., Zhang, H., Xu, J., Liu, Y., Wang, Z., Xing, S., Chen, G., Pan, J., Yu, J., Wang, Y., Wang, L., & Qiao, Y. (2022, December 7). InternVideo: General Video Foundation models via generative and Discriminative Learning. arXiv.org. https://arxiv.org/abs/2212.03191 </div>
        <div class="body-text">Li, K., He, Y., Wang, Y., Li, Y., Wang, W., Luo, P., Wang, Y., Wang, L., & Qiao, Y. (2024, January 4). VideoChat: Chat-centric video understanding. arXiv.org. https://arxiv.org/abs/2305.06355 </div>
        <div class="body-text">Xu, H., Ye, Q., Yan, M., Shi, Y., Ye, J., Xu, Y., Li, C., Bi, B., Qian, Q., Wang, W., Xu, G., Zhang, J., Huang, S., Huang, F., & Zhou, J. (2023, February 1). MPLUG-2: A modularized multi-modal foundation model across text, image and video. arXiv.org. https://arxiv.org/abs/2302.00402 </div>
        <div class="body-text">Team, G., Georgiev, P., Lei, V. I., Burnell, R., Bai, L., Gulati, A., Tanzer, G., Vincent, D., Pan, Z., Wang, S., Mariooryad, S., Ding, Y., Geng, X., Alcober, F., Frostig, R., Omernick, M., Walker, L., Paduraru, C., Sorokin, C., ‚Ä¶ Vinyals, O. (2024, August 8). Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv.org. https://arxiv.org/abs/2403.05530 </div>
        <div class="body-text">Guo, C., Mu, Y., Javed, M. G., Wang, S., & Cheng, L. (2023, November 29). Momask: Generative masked modeling of 3D human motions. arXiv.org. https://arxiv.org/abs/2312.00063 </div>
        <div class="body-text">Zhang, J., Zhang, Y., Cun, X., Huang, S., Zhang, Y., Zhao, H., Lu, H., & Shen, X. (2023, September 24). T2M-GPT: Generating human motion from textual descriptions with discrete representations. arXiv.org. https://arxiv.org/abs/2301.06052 </div>
        <div class="body-text">Petrovich, M., Black, M. J., & Varol, G. (2023, August 25). TMR: Text-to-motion retrieval using contrastive 3D human motion synthesis. arXiv.org. https://arxiv.org/abs/2305.00976 </div>
        <div class="body-text">Generating Diverse and Natural 3D Human Motions from Texts Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, Li Cheng CVPR, 2022</div>
      </div>
      </div>


      

      

    <!-- FOOTER -->
    <div id="footer">
      <!-- 
        TODO - Change href to your Instagram account (can also delete entire "a" element if no Instagram) 

        This should be the same across all pages.
      -->
      <a class="icon-link" target="_blank" href="https://www.linkedin.com/in/chrislizenan/">
        <image src="../assets/icons/linkedin-svgrepo-com.svg" class="footer-icon"/>
      </a>
      <!-- 
        TODO - Change href to your Twitter account (can also delete entire "a" element if no Twitter) 
      
        This should be the same across all pages.
      -->
      <!-- 
        TODO - Change the email after "mailto" to your contact email 
      
        This should be the same across all pages.
      -->
      <a class="icon-link" href="mailto:zenanlicareer@gmail.com">
        <image src="../assets/icons/mail.svg" class="footer-icon"/>
      </a>
    </div>

	</body>
</html>
